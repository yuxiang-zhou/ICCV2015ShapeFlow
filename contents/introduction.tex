Uses Introductions from Nondas' paper "Automatic Construction of Deformable Models In-The-Wild" as place holder.

Many deformable objects exist everywhere around us.
Some examples are the human face and body, animals, vehicles
such as cars and motorcycles and objects of everyday
use like tables, chairs etc. Most of these objects consist of
different parts and appear in instances with great variance
in shape and appearance. Thus, the concept of a deformable
object refers to the deformation of both shape and appearance
of an object. For example cars have parts (i.e. doors,
windows, tires etc.) with significant changes in shape, size
and texture. Furthermore, faces consist of parts (i.e. nose,
eyes, mouth etc.) which not only vary with respect to shape
and appearance, but can demonstrate a number of expressions
due to muscles. Recently, we have witnessed a great
progress in object detection, alignment and recognition.

In order to train deformable models with good generalization
ability, a large amount of carefully annotated data
is needed. Developing useful datasets and benchmarks that
can contribute in the progress of an application domain is
a highly time consuming and costly procedure. It requires
both careful selection of the images, so that they can model
the vast amount of an objectâ€™s variability, and careful annotation
of the various parts of the object (or landmarks).
The amount of annotation that is required depends on both
the object and the application. In faces, for example, where
many landmark points are needed in tasks such as facial expression
analysis, motion capture and expression transfer,
usually more than 60 points are annotated \cite{?}.
To illustrate how much time consuming careful face annotation
is, according to our experience, a trained annotator
may need an average of 8 minutes per image for the manual
annotation of 68 landmarks1. This means that the annotation
of 1000 images requires a total of about 130 hours2.
Furthermore, fatigue can cause errors on the accuracy and
consistency of annotations and they may require correction.

In this paper, we deal with the problem of automatically
constructing a robust deformable model using (1) a simple
bounding box object detector and (2) a shape by means of
a Point Distribution Model (PDM). The detector can be as
simple as the Viola-Jones object detector \cite{?}
3 which returns
only a bounding box of a detected object. Such detectors
are widely employed in commercial products (e.g.
even the cheapest digital camera has a robust face detector).
Other detectors that can be used are efficient subwindow
search \cite{?}. The
annotations that are needed to train the object detector can
be acquired very quickly, since only a bounding box containing the object is required. Specifically, after selecting
the images that are going to be used, the annotation procedure
takes a couple of seconds per image. The statistical
shape model can be created by using only 40-50 shape examples,
which can be produced by either drawing possible
shape variations of the 2D shape of the object or projecting
3D CAD model instances of the object on the 2D camera
plane (such an example is shown in \cite{?} for cars). Even
the annotation of the shape examples is not a time consuming
task, due to their small number. Furthermore, there are
unsupervised techniques to learn the shape prior (model) directly
from images \cite{?}.

Due to the fact that manual annotation is a rather costly
and labour-intensive procedure, unsupervised and semisupervised
learning of models for the tasks of alignment,
landmark localization, tracking and recognition has attracted
considerable attention \cite{?}. In this paper, we
propose a method to automatically construct deformable
models for object alignment and the most related works
are \cite{?}. The related family of techniques,
known as image-congealing \cite{?}, uses implicit
models to align a set of images as a whole, which means
that both performing alignment to a new image and constructing
a model is not straightforward. Our methodology
differs from these works because we employ an explicit texture
model which is learned through the process.

The two most closely related works to the proposed
method are the automatic construction of Active Appearance
Models (AAMs) in \cite{?} and the so-called RASL
methodology in \cite{?} for person-specific face alignment.
There are two main differences between our framework
and \cite{?}. (1) We use a predefined statistical shape model
instead of trying to find both the shape and appearance models.
We believe that with the current available optimization
techniques, it is extremely difficult to simultaneously optimize
for both the texture and shape parameters. (2) We
employ the robust component analysis of \cite{?} for the appearance
which deals with outliers. Thus, even though
our method is similar in concept to \cite{?}, these two differences
make the problem feasible to solve. In particular, the
methodology in \cite{?} fails to create a generic model even in
controlled recording conditions, due to extremely high dimensionality
of the parameters to be found and to the sensitivity
of the subspace method to outliers. This was probably
one of the reasons why the authors demonstrate very
limited and only person-specific experiments. Furthermore,
our methodology bypasses some of the limitations of \cite{?},
which requires the presence of only one low-rank subspace,
hence it has been shown to work only for the case of congealing
images of a single person. Finally, we argue that
in order for an automatically constructed AAM methodology
to be robust to both within-class and out-of-class outliers4,
which cannot be avoided in totally unsupervised settings,
statistical component analysis techniques should be
employed \cite{?}. We summarize our contributions as follows:


We present experimental results on the task of face alignment.
We choose this application because there exist many
in-the-wild facial databases with a large number of images
and annotated landmarks, hence solid quantitative evaluations
can be performed.

\clearpage